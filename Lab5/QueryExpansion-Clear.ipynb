{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Just some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import common as cm\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Simple search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Get acquainted with the below class. There are several TODOs. However, DO NOT complete them now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['aaai', 'about', 'academic', 'access', 'acquired', 'acquisition', 'action', 'activity', 'actual', 'adaptive', 'add', 'advance', 'agricultural', 'aha', 'aim', 'alert', 'algorithm', 'all', 'analysis', 'and', 'announcement', 'answer', 'anyone', 'application', 'applied', 'apply', 'applying', 'approach', 'approache', 'april', 'archive', 'are', 'area', 'areas', 'article', 'artificial', 'asked', 'august', 'author', 'automated', 'automatically', 'autonomous', 'available', 'awards', 'backend', 'backgammon', 'baldi', 'based', 'basic', 'bayesian']\n"
     ]
    }
   ],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        ### keeps unique terms (SORTED!)\n",
    "        self.terms = self.loadTerms(\"terms.txt\")\n",
    "        self.idfs = [] ### IDF coefficients\n",
    "        self.corM = [] ### a correlation matrix\n",
    "\n",
    "    ### load terms\n",
    "    def loadTerms(self, fileName):\n",
    "        file = open(fileName,'r', encoding='utf-8-sig')\n",
    "        k = [self.proc(s) for s in file.readlines()]\n",
    "        k.sort()\n",
    "        return k\n",
    "\n",
    "    ### ignore it\n",
    "    def proc(self, s):\n",
    "        if s[-1] == '\\n': return s[:-1]\n",
    "        else: return s\n",
    "    \n",
    "    ### TODO (DO NOT FINISH THIS METHOD YET. YOU WILL BE ASKED FOR IT LATER) \n",
    "    def computeIDFs(self, documents):\n",
    "        N = len(documents)\n",
    "        idfDict = {}\n",
    "        for document in documents:\n",
    "            for word in document.tokens:\n",
    "                if word not in idfDict: \n",
    "                    idfDict[word] = 1\n",
    "                else:\n",
    "                    idfDict[word] += 1\n",
    "        \n",
    "        for word, val in idfDict.items():\n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "\n",
    "        self.idfs = []\n",
    "        for val in idfDict.values():\n",
    "            self.idfs.append(val)\n",
    "    \n",
    "    ### TODO (DO NOT FINISH THIS METHOD YET. YOU WILL BE ASKED FOR IT LATER) \n",
    "    def computeCorM(self, documents):\n",
    "        self.corM = [[0]]\n",
    "        \n",
    "\n",
    "### SOME DEBUG\n",
    "dictionary = Dictionary()\n",
    "print(dictionary.terms[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Load files: here we load some example collection of documents. RAW_DOCUMENTS = just strings. Check if the documents are loaded correctly (e.g., print RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "David W. Aha:  Machine Learning Page\n Machine Learning Resources. Suggestions welcome. ... (WizRule); ZDM Scientific\n Ltd. Conference Announcements. Courses on Machine Learning. Data Repositories. ... \n Description: Comprehensive machine learning resources from Applications to Tutorials.\n\n"
     ]
    }
   ],
   "source": [
    "RAW_DOCUMENTS = cm.loadDocuments(\"documents.txt\")\n",
    "### SOME DEBUG\n",
    "print(RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['david', 'aha', 'machine', 'learning', 'page', 'machine', 'learning', 'resource', 'suggestion', 'welcome', 'wizrule', 'zdm', 'scientific', 'ltd', 'conference', 'announcement', 'course', 'machine', 'learning', 'data', 'repository', 'description', 'comprehensive', 'machine', 'learning', 'resource', 'from', 'application', 'tutorials']\n"
     ]
    }
   ],
   "source": [
    "### SOME DEBUG, JUST RUN; check if (a) common.py is imported correctly and (b) \n",
    "### tokens are correctly derived from some document (e.g., RAW_DOCUMENTS[0])\n",
    "print(cm.simpleTextProcessing(RAW_DOCUMENTS[0], re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Get acquainted with the below class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['david', 'aha', 'machine', 'learning', 'page', 'machine', 'learning', 'resource', 'suggestion', 'welcome', 'wizrule', 'zdm', 'scientific', 'ltd', 'conference', 'announcement', 'course', 'machine', 'learning', 'data', 'repository', 'description', 'comprehensive', 'machine', 'learning', 'resource', 'from', 'application', 'tutorials']\n"
     ]
    }
   ],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id, raw_document, dictionary):\n",
    "        self.doc_id = doc_id ### DOC ID, simply 0,1,2,3....\n",
    "        self.raw_document = raw_document ### raw data, i.e., string data\n",
    "        self.dictionary = dictionary # reference to the dictionary\n",
    "        \n",
    "        ### DOCUMENT REPRESENTATIONS\n",
    "        self.tokens = cm.simpleTextProcessing(raw_document, re) ### get terms\n",
    "        self.bow = [] # Bag Of Words (BOW - number of term occurences)\n",
    "        self.tf = [] # TF representation\n",
    "        self.tf_idf = [] # TF-IDF representation\n",
    "\n",
    "    ### TODO - complete this method; it should compute a BOW representation\n",
    "    def computeBOW_Representation(self):\n",
    "        #NIE JEST DOBRZE\n",
    "        merge_tokens = ''\n",
    "        for i in self.tokens:\n",
    "            merge_tokens += i+' '\n",
    "\n",
    "        tokenizer.fit_on_texts([merge_tokens])\n",
    "        self.bow = tokenizer.texts_to_matrix([merge_tokens], mode=\"count\")\n",
    "    \n",
    "    ### TODO - complete this method; it should compute a TF representation\n",
    "    def computeTF_Representation(self):\n",
    "        self.tf = []\n",
    "    \n",
    "    ### TODO - complete this method; it should compute a TFxIDF representation \n",
    "    ### (important: it should not be run before dictionary.idfs are not computed!)\n",
    "    def computeTF_IDF_Representation(self):\n",
    "        self.tf_idf = []\n",
    "    \n",
    "    def computeRepresentations(self):\n",
    "        self.computeBOW_Representation()\n",
    "        self.computeTF_Representation()\n",
    "        self.computeTF_IDF_Representation()\n",
    "    \n",
    "documents = [Document(i, RAW_DOCUMENTS[i], dictionary) for i in range(len(RAW_DOCUMENTS))]\n",
    "print(documents[0].tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Compute IDFs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['yahoo', 4.477336814478207], ['york', 4.477336814478207], ['you', 4.477336814478207], ['young', 4.477336814478207], ['zdm', 4.477336814478207]]\n[['access', -0.9563851890760332], ['academic', -0.9343092373768334], ['autonomous', -0.09737416402517636], ['apply', 0.08288765980576765], ['author', 0.7884573603642703]]\n"
     ]
    }
   ],
   "source": [
    "### TODO COMPUTE IDFS HERE (FINISH THE PROPER METHOD OF THE DICTIONARY CLASS - DO NOT FORGET TO RE-RUN THE CELL)\n",
    "dictionary.computeIDFs(documents)\n",
    "#ja https://www.codingame.com/playgrounds/6233/what-is-idf-and-how-is-it-calculated\n",
    "### SOME DEBUG\n",
    "res = [[dictionary.terms[i], dictionary.idfs[i]] for i in range(len(dictionary.terms))]\n",
    "res.sort(key = lambda x: x[1])\n",
    "# LEAST COMMON WORDS - HIGH IDF\n",
    "print(res[-5:])\n",
    "# MOST COMMON WORDS - LOW IDF\n",
    "print(res[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5) Compute the document representations (for each document run computeRepresentations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "es. ... \n\nMachine Learning in Games\n Machine Learning in Games. How computers can learn to get better at playing\n games. This site is for artificial intelligence researchers ... \n Description: How computers can learn to get better at playing games. This site is for artificial intelligence...\n\nMachine Learning in Games has moved\n Machine Learning in Games has moved. Machine Learning in Games is the new location.\n There are many updates, corrections, and improvements at the new site. ... \n Description: Review of research and implementations in Backgammon, Othello, soccer, and other games.\n\nKluwer\n Kluwer Alert, Subscribe today for our free electronic notification service\n for journal tables of contents and new product announcements ... \n\nJournal of Machine Learning Research Homepage\n Description: Provides an international forum for the electronic and paper publication of high-quality scholarly...\n\nIntroduction to Machine Learning\n Introduction to Machine Learning. ... Description (as of October 16, 2001): This\n page has pointers to the various chapters of notes on Machine Learning. ... \n Description: By Nils J. Nilsson (downloadable draft)\n\nUTCS Machine Learning Research Group\n Machine learning is the study of adaptive computational systems that\n improve their performance with experience. The UT Machine Learning ... \n Description: Research on General Inductive Learning, Inductive Logic Programming, Natural Language Learning, Qualitati...\n\nMachine Learning Research Software\n Machine Learning Research Software. We have compiled a group of Common\n Lisp files for various inductive classification algorithms. ... \n\nMachine Learning Project\n Weka Machine Learning Project An exciting and potentially far-reaching development\n in contemporary computer science is the invention and application of methods ... \n\nWeka 3 - Data Mining with Open Source Machine Learning Software ... \n Weka 3: Machine Learning Software in Java. Weka is a collection of machine\n learning algorithms for solving real-world data mining problems. ... \n Description: Weka is a collection of machine learning algorithms for solving real-world data mining problems. It...\n\nMachine Learning Courses\n Index of Machine Learning Courses. Machine Learning,Altay Guvenir, Bilkent University,\n Turkey. Machine Learning, Tony Martinez, Brigham Young University. ... \n\nHome Page of the UW-Madison Machine Learning Research Group\n This page contains relevant information about, and for, the members of the Machine\n Learning Research Group (MLRG) at the University of Wisconsin - Madison. ... \n Description: Research on information retrieval and extraction, bioinformatics, connectionist models, hybrid systems.\n\n1998 International Machine Learning Conference\n (larger version of cover). The Fifteenth International Conference on Machine Learning.\n The on-line schedule for ICML-98 contains links to many of the papers. ... \n Description: The Fifteenth International Conference on Machine Learning. July 24-27, 1998 in Madison, Wisconsin.\n\nMachine Learning\n Machine Learning, THE ... Machine learning refers to a system capable of\n the autonomous acquisition and integration of knowledge. This ... \n\nGMU Machine Learning and Inference Laboratory\n ... 2002 Copyright � 2002-2003 Machine Learning and Inference Laboratory\n Front page created by Guido Cervone and Janejira Kalsmith. ... \n Description: Research on Theories of Learning, Inference, and Discovery Data Mining and Knowledge Discovery, User...\n\nMachine Learning, Neural and Statistical Classification\n Machine Learning, Neural and Statistical Classification. D. Michie, DJ Spiegelhalter,\n CC Taylor (eds). ... Chapter 5: Machine Learning of Rules and Trees. ... \n Description: This book is based on the EC (ESPRIT) project StatLog which compare and evaluated a range of classificat...\n\nYahoo! Groups : machine-learning\n machine-learning � Machine Learning, [ Join This Group! ]. Home, Messages, Links,\n Members Only, Chat, ... Machine Learning mailing list: machine-learning@egroups.com. ... \n Description: An unmoderated mailing list intended for people in computer sciences, statistics, mathematics, and...\n\nYahoo! Science > Computer Science > Artificial Intelligence > ... \n ... Knowledge Sharing Effort, The; Machine Learning and Case-Based Reasoning\n Home Pages; Machine Learning and Inference at GMU; Machine ... \n\nMachine Learning Journal\n Machine Learning Journal. ... Machine Learning publishes articles on the mechanisms\n through which intelligent systems improve their performance over time. ... \n\nThe Machine Learning Systems Group at JPL | Home\n The Home page of JPL's Machine Learning Systems Group, which performs\n applied research in pattern recognition and data mining. ... \n Description: Applied research in data mining, knowledge discovery, pattern recognition, and automated classification...\n\nMachine Learning FAQ\n Machine Learning FAQ. Machine Learning Frequently Asked Questions. Click Here\n to access our page. You are using a Browser that does not support Frames. ... \n Description: Questions and answers about Machine Learning. Anyone can post his/her own questions and answers.\n\nMachine Learning\n 6.858/18.428: Machine Learning. Available Lecture Notes. ... Defining models for\n machine learning. Learning conjunctions in the mistake-bounded model. ... \n\nMachine Learning\n 6.858/18.428: Machine Learning. ... This course deals with the following topics:\n Formal models of machine learning; Learning concepts from examples; ... \n\nArtificial Intelligence Subject Index\n If you came to this page directly from outside, you might want to\n take a look at our general AI Resource page. Machine Learning. ... \n\nNew Link To Robert Holte's home page for Machine Learning\n New Link To Robert Holte's home page for Machine Learning. The web\n pages maintained by Robert Holte for Machine Learning have moved. ... \n\nIFIP WG 12.2 Global Web Site for Machine Learning and Case-Based ... \n IFIP WG 12.2 Global Web Site for Machine Learning and Case-Based Reasoning\n Researchers. You can search for Machine Learning and Case ... \n\nBYU Neural Networks and Machine Learning Lab\n BYU Neural Networks and Machine Learning Laboratory. Lab Director - Tony\n Martinez. ... Links to other Neural Network and Machine Learning resources. ... \n\nMachine Learning\n Machine Learning. Related Sites. Machine Learning Resources courtesy\n of David Aha A Machine Learning Tutorial a good overview of the ... \n\nICML-2001\n The Eighteenth International Conference on Machine Learning (ICML-2001). Williams\n College. ... Previous meetings on machine learning: ICML-2k, ICML-99, ICML-98. ... \n Description: The Eighteenth International Conference on Machine Learning. Williams College (Massachusetts), June...\n\nMachine Learning Group\n [ Bristol CS | Index | Research | ML group | Student projects ] Machine\n Learning Research Group. Overview. The Machine Learning Research ... \n Description: Research on higher-order concept learning, inductive logic programming, multi-agent learning systems,...\n\nTeaching, Machine Learning\n [ Bristol CS | Index | Teaching | Machine Learning Home ] Machine\n Learning and Data Mining Home. Welcome to the home page for the ... \n\nMachine Learning and Information Retrieval (Belew/Shavlik)\n Machine Learning and Information Retrieval. Richard K. Belew. ... Machine Learning:\n Expert Systems and Information Retrieval by Richard Forsyth, Roy Rada. ... \n Description: A collection of online papers about ML approaches to Information Retrieval. Maintained by R. Belew...\n\nICML2002 - Sydney\n The Nineteenth International Conference on Machine Learning (ICML-2002). ... Previous\n meetings on machine learning: ICML-2001, ICML-2000, ICML-99 , ICML-98. ... \n\nThe Machine Learning engine.Dictionary\n The Machine Learning engine.Dictionary for COMP9414. ... Further information on Machine\n Learning can be found in the class web page lecture notes section. ... \n\nICML-2000\n Seventeenth International Conference on Machine Learning. Stanford University. ... Tutorials\n on Commercial Applications of Machine Learning and Data Mining [new]. ... \n Description: Seventeenth International Conference on Machine Learning. Stanford University June 29-July 2, 2000.\n\nMachine Learning and Applied Statistics - Home\n Home Machine Learning and Applies Statistics Overview Group Member Photos People\n Projects Publications MLAS in the News. ... Journal of Machine Learning Research. ... \n Description: The MLAS group is focused on learning from data and data mining. By building software that automatically...\n\nMachine Learning and Applied Statistics - Home\n Home > Research Machine Learning and Applies Statistics Overview Group\n Member Photos People Projects Publications MLAS in the News. Overview. ... \n\nThe Hebrew University - School of Computer Science and ... \n The Machine Learning Lab. Contents: ... Research students: Gill Bejerano Main\n subject : Machine Learning, Applications in Computational Biology. ... \n Description: Research projects on learning in human-machine interaction, natural language interface to the WWW,...\n\nUniversity of York Machine Learning Group\n Content Introduction. \n\nMachine Learning at the University of Toronto\n The Department of Computer Science at the University of Toronto has several\n faculty members working in the area of machine learning, neural networks ... \n\nSGI - MLC++: Home Page\n MLC++ MLC++ is a library of C++ classes for supervised machine learning.\n The MLC++ utilities were created using the library. MLC++ ... \n Description: MLC++ is a standard C++ library for supervised machine learning, with back-end and front-end tools...\n\nGenetics-Based Machine Learning\n These pages are best viewed with some recent version of Netscape. \n Description: Generalization, scheduling and performance evaluation from the Teacher Research Group.\n\nThe NN learning algorithm benchmarking page\n Benchmarking of learning algorithms. information repository page ... Lutz\n Prechelt. Some notes on neural learning algorithm benchmarking. ... \n\nMachine Learning Laboratory\n Machine Learning Laboratory. Overview. In the UMass Machine Learning Laboratory\n (directed by Prof. Utgoff), we study computational ... \n Description: Research on Neural Networks and Decision Trees.\n\nDepartment of Computer Science\n ... advances in areas such as: Machine Learning; Robotics, Computer Vision\n and Wearable Computing; Information Retrieval and Data Mining; ... \n Description: Department news, events, and awards, faculty, staff, grad students home pages, class and program descript...\n\nMachine Learning at UC Santa Cruz\n Machine Learning Home Page. ... The Machine Learning group at UCSC is dedicated to\n the discovery and analysis of algorithms for different learning problems. ... \n Description: Research on decision theory, neural networks, computational biology, computational geometry, theoretical...\n\nMachine Learning Resources on the Web\n Machine Learning Resources on the Web. A refereed electronic journal (ISSN 1093-7609)\n Neural Computing Surveys ... MLC++, a machine learning library in C++. ... \n\nMachine Learning Group\n The Machine Learning Group. Department of Computer and System Sciences\n Stockholm University/KTH Electrum 230 S-164 40 Kista, Sweden ... \n\nThe Rutgers Machine Learning Research Group Homepage\n This page is the main frameset to the Rutgers Machine Learning Research Group\n website. This site uses frames. You can view a noframes version here: \n Description: Research interest in online algorithms, sequence categorization, action prediction, genetic algorithms,...\n\nECML/PKDD-2001 Home\n 12th European Conference on Machine Learning (ECML'01). ... Call for Proposals\n for ECML and/or PKDD 2002. International Society on Machine Learning. ... \n\nMachine Learning and Natural Language Processing Lab\n ... Teaching. Job and Students. Opportunities. Tools and Data. Miscellaneous. Contact.\n Welcome to the chair of. Machine Learning and Natural Language Processing. ... \n Description: Research on Data Mining, Machine Learning,Inductive Logic Programming, Relational Learning, Machine...\n\nMachine Learning for Information Extraction\n AAAI-99 Workshop on Machine Learning for Information Extraction July\n 19, 1999, Orlando Florida. The dramatic growth in the number ... \n\nMachine Learning and Neural Networks Group - Universities of ... \n Machine Learning and Neural Networks group. People. Faculty. Ph.D Students. Paolo\n Frasconi. ... , Machine learning for the web. �, Neural networks for QSPR/QSAR. �, ... \n\nMachine Learning in Bioinformatics\n ... Special Session: Machine Learning in Bioinformatics Call for papers. ... Topics of interest\n include (but are not limited to) applications of machine learning to: ... \n\nCS 661 - Machine Learning (Salzberg)\n CS 661 - Machine Learning (Spring 1996). Steven Salzberg will ... (228K, PostScript).\n On-Line Resources in Machine Learning. Pointers to Machine ... \n\nOpen Directory - Computers: Artificial Intelligence: Machine ... \n ... David W. Aha: Machine Learning Page - Comprehensive machine learning\n resources from Applications to Tutorials. Machine Learning ... \n\nAI / Machine Learning Resources\n AI / Machine Learning Resources. General Machine Learning. The Journal\n of Machine Learning. MLnet Machine Learning Archive at GMD. The ... \n\nMachine Learning in Automated Text Categorization\n Machine Learning in Automated Text Categorization FABRIZIO SEBASTIANI Consiglio Nazionale\n delle Ricerche, Italy The automated categorization (or classification ... \n Description: Survey discussing the main approaches to text categorization that fall within the machine learning...\n\nKernel Machines\n Description: A central source of information on kernel based methods, including support vector machines, Gaussian...\n\nMachine Learning CiteSeer; NEC Research Institute; Steve ... \n ... machine learning researchers that the Bayesian 56 Multiagent Systems: A Survey from\n a Machine Learning Perspective - Stone, Veloso (1997) (Correct) Distributed ... \n\nDTAI - Machine Learning\n ML | DTAI | Dept. of Computer Science | Faculty of Engineering | KU.Leuven,\n DTAI Machine Learning Group. [NEDERLANDS] ENGLISH Machine ... \n\nCWIS: Machine Learning Research Group\n ... Click here to go to the new webpage of the ML group. Machine Learning research\n group. Current research Publications Conferences relevant to us. ... \n\nECML'98 in Chemnitz\n 10th European Conference on Machine Learning. April 21 - 24, 1998. ... Human Learning\n meets Machine Learning\" organized by D. Canamero and M. Van Someren. ... \n\nIDIAP : > MachineLearning > Introduction\n What do we mean by Statistical Machine Learning? ... The actual research\n directions of Statistical Machine Learning are quite diverse. ... \n\nCogprints - Subject: Machine Learning\n Subject: Machine Learning. ... Proceedings AAAI Fall Symposium on Machine Learning and\n Computer Vision, pages 70-74, Research Triangle Park, North Carolina, USA. ... \n\nSimple Machines\n return to previous page. Simple Machines Learning Site. ... Each of the machine\n pages below contain information and activities for the students to use. ... \n\nMachine Learning/Genetic Algorithm group - Dipartimento di ... \n ... The Machine Learning group at the Dipartimento di Informatica, Universita di Torino\n is part of a larger research group on Artificial Intelligence, and is ... \n Description: Research on learning first-order classification rules, first-order concept descriptions, genetic algorith...\n\nECML/PKDD-2002\n ... Photos Proposals 2003 Sitemap Home ECML/PKDD-2002. 13th European Conference\n on Machine Learning (ECML'02). 6th European Conference on ... \n\nMachine Learning for Information Retrieval: Neural Networks, ... \n Machine Learning for Information Retrieval: Neural Networks, Symbolic\n Learning, and Genetic Algorithms. Contents. List of Figures. ... \n\nICML 2003\n The Twentieth International Conference on Machine Learning (ICML-2003).\n August 21-24, 2003 Washington, DC USA. The Twentieth International ... \n\nMachine Learning\n Machine Learning. Machine Learning Home Page (Editor) Machine Learning Home\n Page (Publisher) Machine Learning Online by Kluwer Academic Publishers: ... \n\nAAAI Spring Symposium: Machine Learning in Information Access\n AAAI Spring Symposium on Machine Learning in Information Access. ... Rik Belew has\n collected resources relevant to Machine Learning and Information Access . ... \n\nICML-99\n Description: The Sixteenth International Conference on Machine Learning. June 27-30, 1999, Bled, Slovenia.\n\nTraining - Machine Learning network Online Information Service\n The aim of the MLnet II - Training Committee is to serve as a reference for scientists\n involved in machine learning, as well as to introduce the basic ideas to ... \n\nOxford University Machine Learning Group\n Machine Learning at the Computing Laboratory. ... Logic Programming and\n Learning and Intelligent Systems. Other Machine Learning Groups. ... \n\nIJCAI 99 Workshop: Machine Learning for Information Filtering. Workshop\n on \"Machine Learning for Information Filtering\" at IJCAI 99. ... \n\nData Mining\n By Author. Data Mining: Practical Machine Learning Tools and Techniques with\n Java Implementations Ian H. Witten, Eibe Frank, ... \n\nC4.5: Programs for Machine Learning\n By Author. C4.5: Programs for Machine Learning J. Ross Quinlan, The\n Morgan Kaufmann Series in Machine ... \n\nBioinformatics\n Bioinformatics The Machine Learning Approach. by Pierre Baldi and Soren\n Brunak. MIT Press February 1998 ISBN 0-262-0244-X 360 pp., 62 ... \n Description: by P. Baldi and S. Brunak, MIT Press February 1998.\n\nInternational Conferences on Machine Learning and Applications\n The 2002 International Conferences on Machine Learning and Applications The\n 2003 International Conferences on Machine Learning and Applications. \n\nApplying a Machine Learning Workbench: Experience with ... \n Applying a Machine Learning Workbench: Experience with Agricultural Databases Stephen\n R. Garner, Sally Jo Cunningham, Geoffrey Holmes, Craig G. Nevill-Manning ... \n[[0. 0. 0. 4. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n  0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for d in documents: d.computeRepresentations()\n",
    "### SOME DEBUG (you should see some 4s - which terms are these?)\n",
    "print(documents[0].bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6) Finish the below method. It should compute and return a cosine similarity (v1 and v2 are two vectors - tf-idf representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO \n",
    "def getSimilarity(v1, v2):\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7) Run the below script for different queries. getTopResults seeks for documents being the most similar/relevant to the query. Do you find the results satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"machine learning\"\n",
    "#query = \"academic research\"\n",
    "#query = \"international conference\"\n",
    "#query = \"international conference washington\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopResults(query, documents, dictionary, similarity, top = 5):\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults(query, documents, dictionary, getSimilarity, top = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1) Finish dictionary.computeCorM method (see class Dictionary). It should generate a correlation matrix (correlation between terms).\n",
    "\n",
    "IMPORTANT: although corM[ i ][ i ] (for each i) should be 1.0, set it to -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - COMPLETE THE computeCorM METHOD (see one of the first cells)\n",
    "dictionary.computeCorM(documents)\n",
    "print(dictionary.corM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2) Finish the below method. For each term in the query (you must parse the query, see getTopResults() method), find another term which is the most correlated with the input term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"machine\"\n",
    "#query = \"algorithm\"\n",
    "# query = \"learning\"\n",
    "# query = \"conference\"\n",
    "# query = \"research\"\n",
    "# query = \"concept\"\n",
    "\n",
    "def suggestKeywords(query, dictionary):\n",
    "    ### TODO\n",
    "    print(\"Suggestions\")\n",
    "    pass\n",
    "        \n",
    "suggestKeywords(query, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) Rocchio algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overrightarrow{q_{m}} = \\alpha \\overrightarrow{q} + \\left(\\beta \\cdot \\dfrac{1}{|D_{r}|} \\sum_{\\overrightarrow{D_j} \\in D_{r}} \\overrightarrow{D_j} \\right) - \\left(\\gamma \\cdot \\dfrac{1}{|D_{nr}|} \\sum_{\\overrightarrow{D_j} \\in D_{nr}} \\overrightarrow{D_j} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1) Firstly, run the below code. Observe the results. Assume that we do not like the first and the second result (Docs 63 and 77). However, assume that docs 29 and 36 are satisfactory. Now, modfify the method. It should alter the query vector, according to Rocchio algorithm. Check the result for the above considered scenario (relevant docs = 29 and 36; not relevant = 63 and 77). Check the results for different values of alpha, beta, and gamma coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopResults_Rocchio(query, \n",
    "                          documents, \n",
    "                          dictionary, \n",
    "                          similarity, \n",
    "                          rel_docs = [29, 36],\n",
    "                          nrel_docs = [63, 77],\n",
    "                          alpha = 0.5,\n",
    "                          beta = 0.3,\n",
    "                          gamma = 0.2,\n",
    "                          top = 10):\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    ##### TODO: MODIFY qd.tf_idf HERE\n",
    "    \n",
    "    #####\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults_Rocchio(\"machine learning\", documents, dictionary, getSimilarity, top = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1) Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/install.html\n",
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download()\n",
    "\n",
    "https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: synset = (from wiki) (information science) A set of one or more synonyms that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2) Display sysents for \"machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3) Display all definitions (.definition()) for synsets (machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4) For each synset (machine), display its hypernym (a word with a broad meaning constituting a category into which words with more specific meanings fall; a superordinate. For example, colour is a hypernym of red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See: http://www.nltk.org/howto/wordnet.html\n",
    "for more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}